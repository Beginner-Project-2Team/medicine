# -*- coding: utf-8 -*-
"""ym1_baseline_code_team2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p-ZWxg7TLZZdRtsYCY0DUEVuTjU5dVfH
"""

from google.colab import drive
drive.mount("/content/drive")

import os, zipfile

DATA_ROOT = "/content/drive/MyDrive/sprint_ai_project1_data"
ZIP_PATH  = os.path.join(DATA_ROOT, "train_annotations_clean.zip")

assert os.path.exists(ZIP_PATH), f"zip not found: {ZIP_PATH}"

with zipfile.ZipFile(ZIP_PATH, "r") as z:
    z.extractall(DATA_ROOT)

print("âœ… unzip done")

import os, json

DATA_ROOT = os.environ.get(
    "MEDICINE_DATA_ROOT",
    "/content/drive/MyDrive/sprint_ai_project1_data"
)
ANN_ROOT = os.path.join(DATA_ROOT, "train_annotations_clean")

def collect_unique_cats(ann_root):
    cat_ids = set()
    for dirpath, _, filenames in os.walk(ann_root):
        for fn in filenames:
            if not fn.lower().endswith(".json"):
                continue
            jp = os.path.join(dirpath, fn)
            with open(jp, "r", encoding="utf-8") as f:
                coco = json.load(f)
            for ann in coco["annotations_clean"]:
                cat_ids.add(ann["category_id"])
    return sorted(list(cat_ids))

unique_cats = collect_unique_cats(ANN_ROOT)
print("unique category count:", len(unique_cats))
print("example ids:", unique_cats)

# background = 0, ë‚˜ë¨¸ì§€ í´ë˜ìŠ¤ëŠ” 1~K
catid_to_model = {cat_id: i+1 for i, cat_id in enumerate(unique_cats)}
model_to_catid = {v: k for k, v in catid_to_model.items()}

NUM_CLASSES = 1 + len(unique_cats)
print("NUM_CLASSES:", NUM_CLASSES)

print(invalid_bboxes)

from PIL import Image

IMG_ROOT = os.path.join(DATA_ROOT, "train_images")

def find_json_path(root_dir, filename):
    for dirpath, _, filenames in os.walk(root_dir):
        if filename in filenames:
            return os.path.join(dirpath, filename)
    return None

def show_problem_sample(rec):
    fn_json = rec["file"]

    # [ìˆ˜ì •] ë‹¨ìˆœíˆ os.path.joinì„ ì“°ëŠ” ëŒ€ì‹  find_json_pathë¡œ í•˜ìœ„ í´ë”ê¹Œì§€ ê²€ìƒ‰í•˜ì—¬ ì‹¤ì œ ê²½ë¡œë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    json_path = find_json_path(ANN_ROOT, fn_json)

    # íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°ì— ëŒ€í•œ ì˜ˆì™¸ ì²˜ë¦¬
    if json_path is None:
        print(f"Error: {fn_json} íŒŒì¼ì„ {ANN_ROOT} ë° í•˜ìœ„ í´ë”ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ì°¾ì€ ê²½ë¡œ(json_path)ë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ì„ ì—½ë‹ˆë‹¤.
    with open(json_path, "r", encoding="utf-8") as f:
        coco = json.load(f)

    img_info = coco["images"][0]
    img_path = os.path.join(IMG_ROOT, img_info["file_name"])
    img = Image.open(img_path).convert("RGB")

    import matplotlib.pyplot as plt
    import matplotlib.patches as patches

    fig, ax = plt.subplots(1,1, figsize=(6,6))
    ax.imshow(img); ax.set_axis_off()

    for ann in coco["annotations_clean"]:
        x, y, w, h = ann["bbox"]
        rect = patches.Rectangle(
            (x, y), w, h,
            linewidth=2,
            edgecolor="red",
            facecolor="none",
        )
        ax.add_patch(rect)
    plt.show()

# invalidê°€ ìˆë‹¤ë©´ ì²« ìƒ˜í”Œ í™•ì¸
if invalid_bboxes:
    show_problem_sample(invalid_bboxes[0])

import os
# category_id -> name ë§Œë“¤ê¸°
catid_to_name = {}
for dirpath, _, filenames in os.walk(ANN_ROOT):
    for fn in filenames:
        if not fn.lower().endswith(".json"):
            continue
        jp = os.path.join(dirpath, fn)
        with open(jp, "r", encoding="utf-8") as f:
            coco = json.load(f)
        for cat in coco.get("categories", []):
            catid_to_name[cat["id"]] = cat.get("name", "")

print("Top 10 (id, name, count):")
for cat_id, cnt in cat_freq.most_common(10):
    print(cat_id, catid_to_name.get(cat_id, ""), cnt)

"""# ì„¤ëª…: category_id -> name ë§¤í•‘ ìˆ˜ì§‘

- ëª©ì : ëª¨ë“  ì£¼ì„ íŒŒì¼ì—ì„œ `categories` ë°°ì—´ì„ ì½ì–´ `category_id`ì— í•´ë‹¹í•˜ëŠ” ì´ë¦„(ì˜ˆ: ì•½ ì´ë¦„)ì„ ëª¨ì•„ì„œ `catid_to_name` ë”•ì…”ë„ˆë¦¬ë¥¼ ë§Œë“­ë‹ˆë‹¤.

### ì½”ë“œ ì„¤ëª…
- `for cat in coco.get("categories", [])`:
  - `get`ì„ ì‚¬ìš©í•˜ì—¬ `categories` í‚¤ê°€ ì—†ì„ ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•´ ì•ˆì „í•˜ê²Œ ìˆœíšŒí•©ë‹ˆë‹¤.
- `catid_to_name[cat["id"]] = cat.get("name", "")`:
  - `id`ë¥¼ í‚¤ë¡œ, `name`ì„ ê°’ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤. `name`ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.

### ì‘ì„± ì´ìœ 
- ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼(display)ì—ì„œ í´ë˜ìŠ¤ ì•„ì´ë””ë§Œ ìˆìœ¼ë©´ ì‚¬ëŒì´ í•´ì„í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. ë”°ë¼ì„œ `id->name` ë§¤í•‘ì„ ë§Œë“¤ì–´ ì‹œê°í™”ë‚˜ ê²°ê³¼ ë¬¸ì„œí™” ì‹œ ì›ë¬¸ ì´ë¦„ì„ í‘œì‹œí•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.
"""

import os, json
import torch
from torch.utils.data import Dataset
from PIL import Image
import torchvision.transforms.v2.functional as F
from torchvision.tv_tensors import Image as TvImage, BoundingBoxes

class PillMergedDataset(Dataset):
    def __init__(self, root, split="train", transforms=None):

        self.root = root
        self.split = split
        self.transforms = transforms

        if split == "train":
            self.img_dir = os.path.join(root, "train_images")
            ann_root = os.path.join(root, "train_annotations_clean")

            # file_name -> { "width": ..., "height": ..., "anns": [ann, ...], "cat_names": {cat_id: name} }
            file_to_info = {}

            for dirpath, _, filenames in os.walk(ann_root):
                for fn in filenames:
                    if not fn.lower().endswith(".json"):
                        continue
                    jp = os.path.join(dirpath, fn)
                    with open(jp, "r", encoding="utf-8") as f:
                        coco = json.load(f)

                    img = coco["images"][0]
                    file_name = img["file_name"]
                    width = img["width"]
                    height = img["height"]

                    if file_name not in file_to_info:
                        file_to_info[file_name] = {
                            "width": width,
                            "height": height,
                            "anns": [],
                            "cat_names": {},   # category_id -> name
                        }

                    # categories ë°°ì—´ì—ì„œ id -> name ë§¤í•‘ ì €ì¥
                    for cat in coco.get("categories", []):
                        cid = cat["id"]
                        cname = cat.get("name", "")
                        file_to_info[file_name]["cat_names"][cid] = cname

                    # annotations ì¶”ê°€
                    for ann in coco["annotations"]:
                        file_to_info[file_name]["anns"].append(ann)

            self.filenames = sorted(file_to_info.keys())
            self.file_to_info = file_to_info

        elif split == "test":
            self.img_dir = os.path.join(root, "test_images")
            self.filenames = sorted(
                f for f in os.listdir(self.img_dir)
                if f.lower().endswith(".png")
            )
            self.file_to_info = None

        else:
            raise ValueError(f"Unknown split: {split}")

    def __len__(self):
        return len(self.filenames)

    def _load_image(self, file_name: str):
        path = os.path.join(self.img_dir, file_name)
        img = Image.open(path).convert("RGB")
        return img

    def _load_target(self, file_name: str, image_id: int):
        info = self.file_to_info[file_name]
        anns = info["anns"]
        cat_names_map = info.get("cat_names", {})

        boxes, labels, areas, iscrowd = [], [], [], []
        names = []

        for ann in anns:
            x, y, w, h = ann["bbox"]
            if w <= 0 or h <= 0:
                continue

            orig_cat = ann["category_id"]
            if orig_cat not in catid_to_model:
                continue
            model_label = catid_to_model[orig_cat]

            boxes.append([x, y, x + w, y + h])
            labels.append(model_label)
            areas.append(ann.get("area", w * h))
            iscrowd.append(ann.get("iscrowd", 0))

            # category_idë¡œë¶€í„° ì´ë¦„ ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´)
            pill_name = cat_names_map.get(orig_cat, "")
            names.append(pill_name)

        if len(boxes) == 0:
            boxes = torch.zeros((0, 4), dtype=torch.float32)
            labels = torch.zeros((0,), dtype=torch.int64)
            areas = torch.zeros((0,), dtype=torch.float32)
            iscrowd = torch.zeros((0,), dtype=torch.int64)
            names = []
        else:
            boxes = torch.as_tensor(boxes, dtype=torch.float32)
            labels = torch.as_tensor(labels, dtype=torch.int64)
            areas = torch.as_tensor(areas, dtype=torch.float32)
            iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64)

        target = {
            "boxes": boxes,
            "labels": labels,
            "image_id": torch.tensor([image_id]),
            "area": areas,
            "iscrowd": iscrowd,
            "names": names,   # ì¶”ê°€
        }
        return target



    def __getitem__(self, idx):
        file_name = self.filenames[idx]
        img = self._load_image(file_name)

        image_id = idx  # í•™ìŠµ ë‚´ë¶€ìš© id

        if self.split == "train":
            target = self._load_target(file_name, image_id)
        else:
            target = {"image_id": torch.tensor([image_id])}

        img = TvImage(F.pil_to_tensor(img))  # [C,H,W], uint8

        if self.split == "train":
            target["boxes"] = BoundingBoxes(
                target["boxes"], format="XYXY", canvas_size=F.get_size(img)
            )

        if self.transforms is not None:
            img, target = self.transforms(img, target)

        return img, target

def show_sample(dataset, idx=0):
    img, target = dataset[idx]

    if isinstance(img, TvImage):
        img_pil = F.to_pil_image(img).convert("RGB")
    else:
        img_pil = F.to_pil_image(img)

    boxes = target.get("boxes", None)
    labels = target.get("labels", None)
    names = target.get("names", None)

    if isinstance(boxes, BoundingBoxes):
        boxes = torch.as_tensor(boxes, dtype=torch.float32)

    fig, ax = plt.subplots(1, 1, figsize=(8, 8))
    ax.imshow(img_pil)
    ax.set_axis_off()

    if boxes is not None and len(boxes) > 0:
        for i, box in enumerate(boxes):
            x1, y1, x2, y2 = box.tolist()
            w = x2 - x1
            h = y2 - y1

            rect = patches.Rectangle(
                (x1, y1), w, h,
                linewidth=2,
                edgecolor="red",
                facecolor="none"
            )
            ax.add_patch(rect)

            # pill ì´ë¦„ í…ìŠ¤íŠ¸ êµ¬ì„±
            text = ""
            if names is not None and i < len(names) and names[i]:
                text = names[i]
            elif labels is not None and len(labels) > i:
                text = f"id={int(labels[i])}"

            ax.text(
                x1, max(y1 - 3, 0),
                text,
                fontsize=11,
                color="blue",
                bbox=dict(facecolor="white", edgecolor="white", alpha=0.9, pad=2)
            )

    ax.set_title(f"index={idx}, image_id={int(target['image_id'][0])}")
    plt.tight_layout()
    plt.show()

import matplotlib.pyplot as plt
from matplotlib import font_manager, rc

FONT_PATH = "/content/drive/MyDrive/sprint_ai_project1_data/malgun.ttf"

# 1) í°íŠ¸ íŒŒì¼ì„ matplotlibì— ë“±ë¡
font_manager.fontManager.addfont(FONT_PATH)

# 2) ë“±ë¡ëœ í°íŠ¸ì˜ "ì •í™•í•œ ì´ë¦„" ì–»ê¸°
font_name = font_manager.FontProperties(fname=FONT_PATH).get_name()

# 3) ê·¸ ì´ë¦„ìœ¼ë¡œ í°íŠ¸ ì ìš©
rc("font", family=font_name)
plt.rcParams["axes.unicode_minus"] = False

print("âœ… font loaded:", font_name)

import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib import font_manager, rc

#í•œê¸€ ì„¤ì • - ìœˆë„ìš°
font_path = "/content/drive/MyDrive/sprint_ai_project1_data/malgun.ttf"
font = font_manager.FontProperties(fname=font_path).get_name()
rc('font', family=font)

#ë§ˆì´ë„ˆìŠ¤ ê¹¨ì§ ë°©ì§€
plt.rcParams['axes.unicode_minus'] = False

ds = PillMergedDataset(root=DATA_ROOT, split="train", transforms=None)
for i in range(5):
    show_sample(ds, idx=i)

img, target = ds[3]   # index=3 ì´ë¯¸ì§€
print("boxes:", target["boxes"])
print("labels:", target["labels"])
print("names:", target.get("names"))

import torchvision.transforms.v2 as T

train_transform = T.Compose([
    T.ToDtype(torch.float32, scale=True),        # [0,255] â†’ [0,1]
    T.RandomHorizontalFlip(p=0.5),
    T.RandomResize(min_size=640, max_size=800),  # ì§§ì€ ë³€ ê¸°ì¤€ resize
    T.Normalize(mean=(0.485, 0.456, 0.406),
                std=(0.229, 0.224, 0.225)),
])

test_transform = T.Compose([
    T.ToDtype(torch.float32, scale=True),
    T.Resize(640),
    T.Normalize(mean=(0.485, 0.456, 0.406),
                std=(0.229, 0.224, 0.225)),
])

DATA_ROOT = os.environ.get(
    "MEDICINE_DATA_ROOT",
    "/content/drive/MyDrive/sprint_ai_project1_data"
)

train_dataset = PillMergedDataset(
    root=DATA_ROOT,
    split="train",
    transforms=train_transform
)

test_dataset = PillMergedDataset(
    root=DATA_ROOT,
    split="test",
    transforms=test_transform
)

#Detectionìš© collate_fn. ë¦¬ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ë„˜ê¸°ë„ë¡ ë”°ë¡œ ì •ì˜
def detection_collate_fn(batch):
    """
    batch: [(img1, target1), (img2, target2), ...]
    return: (list_of_images, list_of_targets)
    """
    images = [b[0] for b in batch]
    targets = [b[1] for b in batch]
    return images, targets

from torch.utils.data import DataLoader
BATCH_SIZE = 4

train_loader = DataLoader(
    train_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    collate_fn=detection_collate_fn,

    pin_memory=True,
)

test_loader = DataLoader(
    test_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    collate_fn=detection_collate_fn,
    pin_memory=True,
)

images, targets = next(iter(train_loader))
print(len(images), type(images[0]), images[0].shape)
print(len(targets), targets[0].keys(), targets[0]["boxes"].shape)

import torchvision
from torchvision.models.detection import FasterRCNN
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor

def create_model(num_classes: int):
    # COCO pretrained Faster R-CNN
    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(
        weights="COCO_V1"
    )
    in_features = model.roi_heads.box_predictor.cls_score.in_features

    # num_classes = (í´ë˜ìŠ¤ ìˆ˜ + 1, background í¬í•¨)
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)
    return model

for images, targets in train_loader:
    labels = targets[0]["labels"]
    print("min label:", labels.min().item(), "max label:", labels.max().item())
    break

import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
lr = 0.01

model = create_model(NUM_CLASSES)
model.to(device)

params = [p for p in model.parameters() if p.requires_grad]
optimizer = torch.optim.SGD(
    params,
    lr=lr,
    momentum=0.9,
    weight_decay=0.0005,
)
lr_scheduler = torch.optim.lr_scheduler.StepLR(
    optimizer,
    step_size=11,
    gamma=0.5,
)

#from torch.optim.lr_scheduler import MultiStepLR
#
#scheduler = MultiStepLR(
    #optimizer,
    #milestones=[20, 40],
    #gamma=0.5,
#)
#ëª‡ ì—í­ë§ˆë‹¤ gamma ë°°ìœ¨ë¡œ lr ê°ì†Œí•˜ëŠ”ì§€ ì •í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤

from tqdm import tqdm

def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=50):
    model.train()
    running_loss = 0.0
    for step, (images, targets) in enumerate(tqdm(data_loader, desc=f"Epoch {epoch}")):
        images = [img.to(device) for img in images]
        targets = [{k: v.to(device) if torch.is_tensor(v) else v
                    for k, v in t.items()} for t in targets]

        loss_dict = model(images, targets)
        losses = sum(loss for loss in loss_dict.values())
        loss_value = losses.item()
        running_loss += loss_value

        optimizer.zero_grad()
        losses.backward()
        optimizer.step()

    return running_loss / max(1, len(data_loader))

def train_only(
    model,
    train_loader,
    optimizer,
    lr_scheduler,
    device,
    num_epochs=10,
    print_freq=50,
    save_path="last_model.pth",
):
    for epoch in range(1, num_epochs + 1):
        train_loss = train_one_epoch(
            model, optimizer, train_loader, device, epoch, print_freq=print_freq
        )
        print(f"[Epoch {epoch}] train_loss: {train_loss:.4f}")

        lr_scheduler.step()

    torch.save(model.state_dict(), save_path)
    print(f"Saved last model to {save_path}")

NUM_EPOCHS = 5

train_only(
    model=model,
    train_loader=train_loader,
    optimizer=optimizer,
    lr_scheduler=lr_scheduler,
    device=device,
    num_epochs=NUM_EPOCHS,
    save_path="last_model.pth",
)

torch.cuda.empty_cache()
torch.cuda.ipc_collect()

model.eval()
images, _ = next(iter(test_loader))
images = [img.to(device) for img in images]

with torch.no_grad():
    outputs = model(images)

out = outputs[0]
print("boxes:", out["boxes"].shape)
print("scores:", out["scores"][:10])

import torch

IMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
IMAGENET_STD  = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)

def denormalize(img_tensor):
    """
    img_tensor: [C,H,W], float32, ì •ê·œí™”ëœ í…ì„œ
    ë°˜í™˜: [C,H,W], 0~1 ë²”ìœ„ë¡œ ë³µì›ëœ í…ì„œ
    """
    return img_tensor * IMAGENET_STD + IMAGENET_MEAN

import matplotlib.pyplot as plt
import matplotlib.patches as patches
import torchvision.transforms.v2.functional as F
from torchvision.tv_tensors import Image as TvImage
import torch

def visualize_prediction(image, boxes, labels, scores, names=None,
                         score_thresh=0.3, title=None):
    """
    image: TvImage or tensor [C,H,W]
    boxes: tensor [N,4] (XYXY, CPU)
    labels: tensor [N]
    scores: tensor [N]
    names:  list[str] or None  (ì˜ˆì¸¡ í´ë˜ìŠ¤ ì´ë¦„)
    """
    # image: TvImage or tensor [C,H,W] (ì •ê·œí™”ëœ ìƒíƒœ)

    if isinstance(image, TvImage):
        img_t = torch.as_tensor(image, dtype=torch.float32)
    else:
        img_t = image.clone().detach()

    # ì—­ì •ê·œí™”
    img_t = denormalize(img_t)
    img_t = img_t.clamp(0, 1)

    img_pil = F.to_pil_image(img_t).convert("RGB")

    boxes = boxes.cpu()
    labels = labels.cpu()
    scores = scores.cpu()

    fig, ax = plt.subplots(1, 1, figsize=(6, 6))
    ax.imshow(img_pil)
    ax.set_axis_off()

    for i, box in enumerate(boxes):
        if scores[i] < score_thresh:
            continue

        x1, y1, x2, y2 = box.tolist()
        w = x2 - x1
        h = y2 - y1

        rect = patches.Rectangle(
            (x1, y1), w, h,
            linewidth=2,
            edgecolor="cyan",
            facecolor="none",
        )
        ax.add_patch(rect)

        if names is not None and i < len(names):
            text = f"{names[i]} {scores[i]:.2f}"
        else:
            text = f"id={int(labels[i])} {scores[i]:.2f}"

        ax.text(
            x1, y1 - 5,
            text,
            fontsize=9,
            color="blue",
            fontweight="bold",
            bbox=dict(facecolor="white", edgecolor="white", alpha=0.9, pad=2),
        )

    if title:
        ax.set_title(title)
    plt.tight_layout()
    plt.show()

@torch.no_grad()
def show_test_predictions(model,
                          test_loader,
                          test_dataset,
                          model_to_catid,
                          catid_to_name,   # category_id -> ì•½ ì´ë¦„ dict
                          device,
                          max_images=5,
                          score_thresh=0.3):
    """
    catid_to_name: {category_id: "ë ˆì¼ë¼ì •", ...}
    """
    model.eval()
    model.to(device)

    shown = 0
    idx_offset = 0

    for images, targets in test_loader:
        images = [img.to(device) for img in images]
        outputs = model(images)  # list[dict]

        for i, output in enumerate(outputs):
            if shown >= max_images:
                return

            ds_idx = idx_offset + i
            file_name = test_dataset.filenames[ds_idx]

            boxes = output["boxes"]
            labels = output["labels"]
            scores = output["scores"]

            # ëª¨ë¸ label(1~K) -> ì›ë˜ category_id -> ì´ë¦„
            names = []
            for lab in labels.cpu().tolist():
                cat_id = model_to_catid.get(int(lab), None)
                if cat_id is not None and cat_id in catid_to_name:
                    names.append(catid_to_name[cat_id])
                else:
                    names.append(f"id={lab}")

            # ì‹œê°í™”
            visualize_prediction(
                image=images[i].cpu(),
                boxes=boxes,
                labels=labels,
                scores=scores,
                names=names,
                score_thresh=score_thresh,
                title=f"{file_name}",
            )

            shown += 1

        idx_offset += len(images)

# train_annotations ì „ì²´ë¥¼ ëŒë©´ì„œ category_id -> name ë§¤í•‘ ë§Œë“¤ê¸°
catid_to_name = {}
for dirpath, _, filenames in os.walk(ANN_ROOT):
    for fn in filenames:
        if not fn.lower().endswith(".json"):
            continue
        jp = os.path.join(dirpath, fn)
        with open(jp, "r", encoding="utf-8") as f:
            coco = json.load(f)
        for cat in coco.get("categories", []):
            catid_to_name[cat["id"]] = cat.get("name", "")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.load_state_dict(torch.load("last_model.pth", map_location=device))

min_score = 0.5

show_test_predictions(
    model=model,
    test_loader=test_loader,
    test_dataset=test_dataset,
    model_to_catid=model_to_catid,
    catid_to_name=catid_to_name,
    device=device,
    max_images=30,
    score_thresh=min_score,
)

show_test_predictions(
    model=model,
    test_loader=train_loader,
    test_dataset=train_dataset,
    model_to_catid=model_to_catid,
    catid_to_name=catid_to_name,
    device=device,
    max_images=20,
    score_thresh=min_score,
)

import torch
from collections import defaultdict
from tqdm import tqdm

from torchvision.ops import box_iou

from torchvision.models.detection._utils import Matcher
from torchvision.ops import box_iou
import numpy as np

@torch.no_grad()
def evaluate_map(model, data_loader, device, score_thresh=0.05):
    model.eval()
    model.to(device)

    iou_thresholds = np.arange(0.5, 1.0, 0.05)
    aps = []

    for iou_thresh in iou_thresholds:
        tp, fp, fn = 0, 0, 0

        for images, targets in tqdm(data_loader, desc=f"Evaluating IoU={iou_thresh:.2f}"):
            images = [img.to(device) for img in images]
            outputs = model(images)

            for output, target in zip(outputs, targets):
                gt_boxes = target["boxes"].cpu()
                gt_labels = target["labels"].cpu()

                pred_boxes = output["boxes"].cpu()
                pred_scores = output["scores"].cpu()
                pred_labels = output["labels"].cpu()

                # score threshold
                keep = pred_scores >= score_thresh
                pred_boxes = pred_boxes[keep]
                pred_labels = pred_labels[keep]

                if len(gt_boxes) == 0:
                    fp += len(pred_boxes)
                    continue

                if len(pred_boxes) == 0:
                    fn += len(gt_boxes)
                    continue

                ious = box_iou(pred_boxes, gt_boxes)

                matched_gt = set()
                for i in range(len(pred_boxes)):
                    max_iou, gt_idx = ious[i].max(0)
                    if max_iou >= iou_thresh and gt_idx.item() not in matched_gt:
                        tp += 1
                        matched_gt.add(gt_idx.item())
                    else:
                        fp += 1

                fn += len(gt_boxes) - len(matched_gt)

        precision = tp / max(tp + fp, 1)
        recall = tp / max(tp + fn, 1)
        ap = precision * recall
        aps.append(ap)

    mean_ap = np.mean(aps)

    return {
        "AP@[0.50:0.95]": mean_ap,
        "AP50": aps[0],
        "AP75": aps[5],
    }

metrics = evaluate_map(
    model=model,
    data_loader=train_loader,   # or val_loader
    device=device,
    score_thresh=0.3,
)

print("ğŸ“Š mAP results")
for k, v in metrics.items():
    print(f"{k}: {v:.4f}")